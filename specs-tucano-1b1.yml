################### Directory settings ###################
checkpoint_dir: "/lustre/mlnvme/data/the_tucanos/checkpoints"
train_dataset_dir: "/lustre/mlnvme/data/the_tucanos/gigaverbo-tokenized/train"
val_dataset_dir: "/lustre/mlnvme/data/the_tucanos/gigaverbo-tokenized/val"
cache_dir: "/lustre/mlnvme/data/the_tucanos/cache"

################### Data loading settings ###################
pin_memory: true
num_workers_for_dataloader: 16

################### Model architecture settings ###################
vocab_size: 32000
num_hidden_layers: 22
num_attention_heads: 32
num_key_value_heads: 4
hidden_size: 2048
intermediate_size: 5632
max_position_embeddings: 2048
hidden_act: "silu"
output_hidden_states: false
use_cache: false

################### Training settings ###################
total_batch_size: 524288
micro_batch_size: 16
eval_micro_batch_size: 16
num_train_epochs: 1.5
warmup_steps: 1000
max_learning_rate: 0.0004
min_learning_rate: 0.00004
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
eps: 0.00000001
lr_decay_iters_coef: 0.9
seed: 1337

################### Precision and optimization settings ###################
mat_mul_precision: "highest"
tf32: true
bf16: true
gradient_checkpointing: false

################### Hub settings ###################
push_to_hub: true
hub_token: null
hub_model_id: "TucanoBR/Tucano-1b1"

################### Tokenizer settings ###################
tokenizer_name: "TucanoBR/Tucano-1b1"

################### Checkpoint settings ###################
resume_from_checkpoint: null
checkpointing_steps: 20000

################### Miscellaneous settings ###################
sanity_check: false
wandb_token: null
wandb_id: "Tucano-1b1"