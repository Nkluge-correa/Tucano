################### Directory settings ###################
checkpoint_dir: "/lustre/mlnvme/data/the_tucanos/checkpoints-llama"
train_dataset_dir: "/lustre/mlnvme/data/the_tucanos/sft-tokenized/train"
val_dataset_dir: "/lustre/mlnvme/data/the_tucanos/sft-tokenized/val"
cache_dir: "/lustre/mlnvme/data/the_tucanos/cache"

################### Data loading settings ###################
pin_memory: true
num_workers_for_dataloader: 16

################### Model architecture settings ###################
vocab_size: 32002
num_hidden_layers: 24
num_attention_heads: 16
num_key_value_heads: 4
hidden_size: 2560
intermediate_size: 10240
max_position_embeddings: 4096
hidden_act: "silu"
output_hidden_states: false
use_cache: false

################### Training settings ###################
total_batch_size: 262144
micro_batch_size: 4
eval_micro_batch_size: 4
num_train_epochs: 4
warmup_steps: 0
max_learning_rate: 0.00002
min_learning_rate: 0.000002
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
eps: 0.00000001
lr_decay_iters_coef: 0.9
seed: 1337

################### Precision and optimization settings ###################
mat_mul_precision: "highest"
tf32: true
bf16: true
gradient_checkpointing: false

################### Hub settings ###################
push_to_hub: false
hub_token: null
hub_model_id: "TucanoBR/Tucano-2b4-Instruct"

################### Tokenizer settings ###################
tokenizer_name: "TucanoBR/Tucano-1b1-Instruct"

################### Checkpoint settings ###################
resume_from_checkpoint: "path/to/checkpoint"
checkpointing_steps: 2000

################### Miscellaneous settings ###################
sanity_check: false
wandb_token: null
wandb_id: "Tucano-2b4-instruct"